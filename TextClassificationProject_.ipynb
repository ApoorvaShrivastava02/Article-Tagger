{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import re\n",
    "import nltk \n",
    "import numpy as np\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(rootDir):\n",
    "# This function returns the filenames and categories of articles in the given directory\n",
    "\n",
    "# categories will store the names of all type of articles\n",
    "# documents_count stores the no. of documents in a particular category\n",
    "# docslist stores the list of all documents\n",
    "    categories =[]\n",
    "    docslist = []\n",
    "    dirnamelist = []\n",
    "\n",
    "    for dirName, subdirList, fileList in os.walk(rootDir):\n",
    "\n",
    "        if len(subdirList) >0:\n",
    "            categories = subdirList\n",
    "\n",
    "        if len(subdirList) == 0:\n",
    "            docslist.append(fileList)\n",
    "            dirnamelist.append(dirName)\n",
    "\n",
    "    return categories,docslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_data(docslist,rootDir,categories):\n",
    "# This function reads all the documents in the given directory and cleans the text data and generates tokens\n",
    "# vocab_list stores the vocabulary of our dtaset    \n",
    "# cateogory_docs stores the words per document per category\n",
    "    vocab_list = []\n",
    "    category_docs = []\n",
    "\n",
    "    for index,dlist in enumerate(docslist):\n",
    "        \n",
    "        #docs_words stores the filtered words of documents in a category\n",
    "        \n",
    "        docs_words = []\n",
    "        for doc in dlist:\n",
    "            \n",
    "            \n",
    "            f=open('{0}/{1}/{2}'.format(rootDir,categories[index],doc),'r')\n",
    "\n",
    "            wlist = f.read()\n",
    "            \n",
    "            #using regular expression to remove special and numeric characters from text document\n",
    "            wlist = re.sub('[^A-Za-z]+', ' ', wlist)\n",
    "            wlist = re.sub(r'\\b\\w{1,2}\\b', '', wlist)\n",
    "            wlist = re.sub(r'\\w*\\d\\w*', '', wlist).strip()\n",
    "            word_list = re.findall(r\"[\\w']+\", wlist)\n",
    "            tokens = [token.lower() for token in word_list]\n",
    "            \n",
    "            #removing stopwords from the tokens\n",
    "            filtered_words = [word for word in tokens if word not in stopwords.words('english')]\n",
    "            \n",
    "            #removing words with length less than 3.\n",
    "            for lword in filtered_words:\n",
    "                if len(lword) < 3:\n",
    "                    filtered_words.remove(lword)\n",
    "\n",
    "            \n",
    "            \n",
    "            #appending unique words from the filtered list to our vocabulary list\n",
    "            for uword in filtered_words:\n",
    "                if uword not in vocab_list:\n",
    "                    vocab_list.append(uword)\n",
    "\n",
    "            docs_words.append(filtered_words)\n",
    "        category_docs.append(docs_words)\n",
    "        \n",
    "\n",
    "    return vocab_list,category_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_dataset(vocab_list,categories,category_docs):\n",
    "    # this functions returns a dataframe with frequency of each word in the vocabulary in all the documents\n",
    "    # data stores the frequency data\n",
    "    # target stores the category of each document\n",
    "    # feature_frequency is a dictionary which stores the total frequency of every word in the vocabulary\n",
    "    data=[]\n",
    "    target=[]\n",
    "    feature_frequency={}\n",
    "    count=0\n",
    "    for category_index in range(0,len(category_docs)):\n",
    "        \n",
    "        \n",
    "        for doc in range(0,len(category_docs[category_index])):\n",
    "            target.append(categories[category_index])\n",
    "            data.append(list(np.zeros(len(vocab_list),int)))\n",
    "            \n",
    "            for word in category_docs[category_index][doc]:\n",
    "                \n",
    "                index=vocab_list.index(word)\n",
    "                data[count][index]+=1\n",
    "                if word in feature_frequency.keys():\n",
    "                    feature_frequency[word]+=1\n",
    "                else:\n",
    "                    feature_frequency[word]=1\n",
    "            count+=1\n",
    "    data_df=pd.DataFrame(data)\n",
    "    data_df.columns=vocab_list\n",
    "    return data_df,target,feature_frequency  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_features(data_df,feature_frequency,vocab_list):\n",
    "    #this function eliminates words and their frequency data from the vocabulary_list and data_df with small overall frequencies\n",
    "    # and takes top 1000 words as final dataset\n",
    "    new=sorted(feature_frequency.items(), key=lambda t:t[1], reverse=True)\n",
    "    feature_frequency=new[:1000]\n",
    "    final_data=pd.DataFrame()\n",
    "    final_vocab=[]\n",
    "    for i in dict(feature_frequency).keys():\n",
    "        final_data[i]=data_df[i]\n",
    "        final_vocab.append(i)\n",
    "    return final_data,final_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X_train,Y_train):#data to be passed as dataframe\n",
    "    # this function returns a dictionary with category-wise the overall frequency of each word in vocabulary\n",
    "    result={}\n",
    "    classes=set(Y_train[0])\n",
    "    for class_ in classes:\n",
    "        result[class_]={}\n",
    "        result[\"total-docs\"]=len(Y_train[0])\n",
    "        for feature in X_train.columns:\n",
    "            result[class_][feature]=X_train[feature][Y_train[0]==class_].sum()\n",
    "        result[class_][\"total_words\"]=sum(result[class_].values())\n",
    "        result[class_][\"class_frequency\"]=len(Y_train[Y_train[0]==class_])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability(dictionary, x, current_class):\n",
    "    output=np.log(dictionary[current_class][\"class_frequency\"])-np.log(dictionary[\"total-docs\"])\n",
    "    \n",
    "    num_features = len(list(dictionary[current_class].keys())[:1000])\n",
    "    for j in range(0, num_features):\n",
    "        xj = x[j]\n",
    "        if xj!=0:\n",
    "            count_current_class_with_value_xj = dictionary[current_class][list((dictionary[current_class].keys()))[j]] + 1\n",
    "            count_current_class = dictionary[current_class][\"total_words\"] + len(dictionary[current_class].keys())\n",
    "            current_xj_probablity = np.log(count_current_class_with_value_xj) - np.log(count_current_class)\n",
    "            output = output + current_xj_probablity\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictSinglePoint(dictionary, x):\n",
    "    #this function predicts the category of a single document\n",
    "    #best_prob is the maximum prbability out of all the probabilities of the classes to be the category of the document.\n",
    "    #best_class is the best prediction for the document(decided on the basis of best_probability)\n",
    "    \n",
    "    classes = list(dictionary.keys())\n",
    "    classes.remove('total-docs')\n",
    "    best_prob = -math.inf\n",
    "    best_class = -1\n",
    "    first_run = True\n",
    "    for current_class in classes:\n",
    "        #p_current_class stores the probability of current class\n",
    "        p_current_class = probability(dictionary, x, current_class)\n",
    "        \n",
    "        if (first_run or p_current_class > best_prob):\n",
    "            best_prob = p_current_class\n",
    "            best_class = current_class\n",
    "        first_run = False\n",
    "    return best_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(dictionary,X_test):\n",
    "    #this functions predicts the category of the documents passed in the dataset\n",
    "    y_pred=[]\n",
    "    x_t=np.array(X_test)\n",
    "    i=0\n",
    "    for x in x_t:\n",
    "        x_class=predictSinglePoint(dictionary, x)\n",
    "        y_pred.append(x_class)\n",
    "        i+=1\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:   0.5505285263061523\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "categories ,docslist=get_filenames(r\"C:\\Users\\nEW u\\ML_A\\Machine Learning\\Naive Bayes\\Text Classification\\mini_newsgroups\")\n",
    "end=time.time()\n",
    "print(\"time taken:  \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:   393.06668996810913\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "vocab_list,category_docs=get_clean_data(docslist,r\"C:\\Users\\nEW u\\ML_A\\Machine Learning\\Naive Bayes\\Text Classification\\mini_newsgroups\",categories)\n",
    "end=time.time()\n",
    "print(\"time taken:  \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:   578.745813369751\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "data_df,target,feature_frequency=frequency_dataset(vocab_list,categories,category_docs)\n",
    "end=time.time()\n",
    "print(\"time taken:  \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:   0.6435110569000244\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "final_data,final_vocab=eliminate_features(data_df,feature_frequency,vocab_list)\n",
    "end=time.time()\n",
    "print(\"time taken:  \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "X_train,X_test,Y_train,Y_test = model_selection.train_test_split(final_data,pd.DataFrame(target),test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predicting the test data using sklearn Multinomial Naive Base\n",
    "clf=MultinomialNB()\n",
    "clf.fit(X_train,Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  13.503151416778564\n"
     ]
    }
   ],
   "source": [
    "#fiting the dataset on the implemented Multinomial Naive Baeyes function\n",
    "start=time.time()\n",
    "dictionary=fit(X_train,Y_train)\n",
    "end=time.time()\n",
    "print(\"time taken: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken:  12.204582214355469\n"
     ]
    }
   ],
   "source": [
    "#predicting for X_test\n",
    "start=time.time()\n",
    "y_pred=predict(dictionary,X_test)\n",
    "end=time.time()\n",
    "print(\"time taken: \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7725"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score for prediction by implemented MultinomialNB\n",
    "accuracy_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPARISON:\n",
    "The accuracy score of the in-built sklearn algorithm and my implementation is exactly the same, i.e, 0.7725"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
